from pathlib import Path

_pipe_cache = {}
_t2i_cache = {}

def load_pipeline(model_dir: Path, device: str, config: dict | None = None):
    import openvino_genai as ov_genai
    import os
    os.environ.setdefault("OPENVINO_LOG_LEVEL", "0")
    target_dir = model_dir
    src_dir = model_dir
    try:
        if not (target_dir / "openvino_model.xml").exists():
            cand = model_dir.parent / (model_dir.name + "_ov_fp32")
            if (cand / "openvino_model.xml").exists():
                target_dir = cand
            else:
                try:
                    from backend.services.inference import export_model_ir as _export
                    _export(model_dir, cand)
                    target_dir = cand if (cand / "openvino_model.xml").exists() else model_dir
                except Exception:
                    target_dir = model_dir
        else:
            try:
                binf = target_dir / "openvino_model.bin"
                need_fallback = (not binf.exists())
                if not need_fallback:
                    try:
                        need_fallback = (binf.stat().st_size <= 0)
                    except Exception:
                        need_fallback = True
                if need_fallback:
                    base = model_dir.name.split("_quant_", 1)[0]
                    cand = model_dir.parent / (base + "_ov_fp32")
                    if (cand / "openvino_model.xml").exists() and (cand / "openvino_model.bin").exists():
                        target_dir = cand
                    else:
                        try:
                            from backend.services.inference import export_model_ir as _export
                            _export(model_dir, cand)
                            if (cand / "openvino_model.bin").exists():
                                target_dir = cand
                        except Exception:
                            pass
            except Exception:
                pass
    except Exception:
        target_dir = model_dir
    tok_xml = target_dir / "openvino_tokenizer.xml"
    if not tok_xml.exists():
        try:
            from transformers import AutoTokenizer
            from openvino_tokenizers import convert_tokenizer
            import openvino as ov
            has_tok = any((src_dir / n).exists() for n in ("tokenizer.json","tokenizer_config.json","vocab.json","merges.txt"))
            if not has_tok:
                base_name = src_dir.name.split("_quant_")[0]
                alt = src_dir.parent / base_name
                if any((alt / n).exists() for n in ("tokenizer.json","tokenizer_config.json","vocab.json","merges.txt")):
                    src_dir = alt
            hf_tok = AutoTokenizer.from_pretrained(str(src_dir), trust_remote_code=True)
            ov_tok, ov_detok = convert_tokenizer(hf_tok, with_detokenizer=True)
            ov.save_model(ov_tok, str(tok_xml))
            ov.save_model(ov_detok, str(target_dir / "openvino_detokenizer.xml"))
        except Exception:
            pass
    import os
    def _ordered_gpu_list(core):
        try:
            avail = core.available_devices
        except Exception:
            avail = []
        gpu_devs = [d for d in avail if d.startswith("GPU")]
        if not gpu_devs:
            return []
        info = {}
        for d in gpu_devs:
            try:
                fn = core.get_property(d, "FULL_DEVICE_NAME")
                fn = str(fn) if fn is not None else ""
            except Exception:
                fn = ""
            info[d] = fn
        def _is_igpu(name):
            if not name:
                return False
            ln = name.lower()
            if "(igpu)" in ln or " igpu" in ln:
                return True
            for k in ("integrated", "uhd", "iris", "hd graphics"):
                if k in ln:
                    return True
            return False
        integrated = [d for d, n in info.items() if _is_igpu(n)]
        if integrated:
            try:
                integrated.sort(key=lambda s: int(s.split(".")[1]) if "." in s and s.split(".")[1].isdigit() else 0)
            except Exception:
                pass
            others = [d for d in gpu_devs if d not in integrated]
            return integrated + others
        try:
            import platform, subprocess
            if platform.system() == "Windows":
                out = subprocess.check_output(["wmic", "path", "Win32_VideoController", "get", "Name"], stderr=subprocess.STDOUT, shell=True, text=True)
                controllers = [ln.strip() for ln in out.splitlines() if ln.strip() and not ln.strip().lower().startswith("name")]
                intel_names = [n for n in controllers if "intel" in n.lower()]
                if intel_names:
                    key = intel_names[0].lower()
                    matched = [d for d, fn in info.items() if key in fn.lower()]
                    if matched:
                        others = [d for d in gpu_devs if d not in matched]
                        return matched + others
        except Exception:
            pass
        intel_list = [d for d, fn in info.items() if fn and "intel" in fn.lower()]
        if intel_list:
            others = [d for d in gpu_devs if d not in intel_list]
            return intel_list + others
        return gpu_devs
    try:
        from pathlib import Path as _P
        _base = os.environ.get("AIFUNLAND_CACHE_DIR") or str(_P.cwd() / "tmp")
        _cd = _P(_base) / "ov_cache"
        _cd.mkdir(parents=True, exist_ok=True)
        os.environ["OV_CACHE_DIR"] = str(_cd)
        os.environ.setdefault("OMP_WAIT_POLICY", "PASSIVE")
        os.environ.setdefault("CUDA_VISIBLE_DEVICES", "")
    except Exception:
        pass
    if config and (device.startswith("AUTO") or device.startswith("MULTI")):
        try:
            from openvino import Core as _Core
            _core = _Core()
            _devs = _core.available_devices
            ordered_gpus = _ordered_gpu_list(_core)
            igpu = ordered_gpus[0] if ordered_gpus else None
            if config.get("hetero_enable"):
                prio = []
                if "NPU" in _devs:
                    prio.append("NPU")
                if igpu:
                    prio.append(igpu)
                if "CPU" in _devs:
                    prio.append("CPU")
                if prio:
                    device = f"HETERO:{','.join(prio)}"
        except Exception:
            pass
    inference_props = {}

    if device == "NPU" or ("NPU" in device):
        inference_props["NUM_STREAMS"] = "1"
    
    import os
    # apply device-specific performance hints
    perf_mode = None
    if config:
        perf_mode = config.get("perf_mode")
    if (perf_mode is None) or (str(perf_mode).upper() == "AUTO"):
        perf_mode = "CUMULATIVE_THROUGHPUT"
    
    # enforce low-latency iGPU+NPU pipeline when requested
    try:
        if config and config.get("prefill_igpu_decode_npu"):
            try:
                from openvino import Core as _Core
                _core = _Core()
                ordered_gpus = _ordered_gpu_list(_core)
                igpu = ordered_gpus[0] if ordered_gpus else "GPU"
            except Exception:
                igpu = "GPU"
            # force device order to GPU first then NPU then CPU
            if device.startswith("HETERO:"):
                device = f"HETERO:{igpu},NPU"
            perf_mode = "LATENCY"
            inference_props["PERFORMANCE_HINT"] = "LATENCY"
            inference_props["NUM_STREAMS"] = "1"
            inference_props["NUM_REQUESTS"] = "1"
            inference_props["NPU_RUN_INFERENCES_SEQUENTIALLY"] = "YES"
            inference_props["MODEL_DISTRIBUTION_POLICY"] = "PIPELINE_PARALLEL"
    except Exception:
        pass
    
    try:
        if device.startswith("HETERO:") and ("NPU" in device) and ("GPU" in device):
            if str(perf_mode).upper() != "LATENCY":
                perf_mode = "LATENCY"
    except Exception:
        pass
    
    try:
        if perf_mode in ("LATENCY", "THROUGHPUT", "CUMULATIVE_THROUGHPUT"):
            inference_props["PERFORMANCE_HINT"] = perf_mode
    except Exception:
        pass
        
    if device == "NPU" or ("NPU" in device):
        streams = None
        if config:
            streams = config.get("npu_streams")
        tiles = None
        num_req = None
        if config:
            tiles = config.get("npu_tiles")
            num_req = config.get("num_requests")
        if streams:
            inference_props["NUM_STREAMS"] = str(streams)
        else:
            if "NUM_STREAMS" not in inference_props:
                 inference_props["NUM_STREAMS"] = "1"
        
        if perf_mode in ("LATENCY", "THROUGHPUT", "CUMULATIVE_THROUGHPUT"):
            inference_props["PERFORMANCE_HINT"] = perf_mode
        else:
            if "PERFORMANCE_HINT" not in inference_props:
                inference_props["PERFORMANCE_HINT"] = "LATENCY"
        
        try:
            ov_mode = "latency" if perf_mode in (None, "LATENCY") else "efficiency"
            inference_props["NPU_COMPILATION_MODE_PARAMS"] = f"optimization-level=2 performance-hint-override={ov_mode}"
            inference_props["NPU_TURBO"] = "YES"
            inference_props["NPU_COMPILER_DYNAMIC_QUANTIZATION"] = "YES"
            inference_props["NPU_RUN_INFERENCES_SEQUENTIALLY"] = "YES" if ov_mode == "latency" else "NO"
            
            if tiles:
                inference_props["NPU_TILES"] = str(tiles)
            if num_req:
                inference_props["NUM_REQUESTS"] = str(num_req)
            else:
                if "NUM_REQUESTS" not in inference_props:
                     inference_props["NUM_REQUESTS"] = "6"
            
            # detect NPU architecture to set tiles and num_requests
            try:
                from openvino import Core
                core = Core()
                arch = core.get_property("NPU", "DEVICE_ARCHITECTURE")
                arch_s = str(arch).lower()
                if "4000" in arch_s:
                    inference_props["NPU_TILES"] = "4"
                    if perf_mode in ("THROUGHPUT", "CUMULATIVE_THROUGHPUT"):
                        inference_props["NUM_REQUESTS"] = "8"
                    else:
                        inference_props["NUM_REQUESTS"] = "1"
                else:
                    # default for 3720/3700
                    inference_props["NPU_TILES"] = "2"
                    if perf_mode in ("THROUGHPUT", "CUMULATIVE_THROUGHPUT"):
                        inference_props["NUM_REQUESTS"] = "4"
                    else:
                        inference_props["NUM_REQUESTS"] = "1"
            except Exception:
                # fallback num_requests for unknown arch
                if "NUM_REQUESTS" not in inference_props:
                    inference_props["NUM_REQUESTS"] = "1" if ov_mode == "latency" else "4"
            
            try:
                enable_prof = False
                if config:
                    v = config.get("enable_profiling")
                    enable_prof = bool(v)
                inference_props["ENABLE_PROFILING"] = "YES" if enable_prof else "NO"
            except Exception:
                inference_props["ENABLE_PROFILING"] = "NO"
        except Exception:
            pass
            
    elif device == "GPU" or ("GPU" in device):
        streams = None
        if config:
            streams = config.get("gpu_streams")
        if streams:
            inference_props["NUM_STREAMS"] = str(streams)
        else:
            if "NUM_STREAMS" not in inference_props:
                inference_props["NUM_STREAMS"] = "1" if perf_mode in (None, "LATENCY") else "2"
        if perf_mode in ("LATENCY", "THROUGHPUT", "CUMULATIVE_THROUGHPUT"):
             inference_props["PERFORMANCE_HINT"] = perf_mode
             
    elif device == "CPU" or ("CPU" in device):
        try:
            nt = os.cpu_count() or 4
            inference_props["INFERENCE_NUM_THREADS"] = str(max(2, nt // 2))
        except Exception:
            pass

    key = (str(model_dir), device)
    p = _pipe_cache.get(key)
    if p is None:
        def _try(dev_str):
            pipe_cfg = {}
            try:
                from pathlib import Path as _P
                _base = os.environ.get("AIFUNLAND_CACHE_DIR") or str(_P.cwd() / "tmp")
                _cd = _P(_base) / "ov_cache"
                pipe_cfg["CACHE_DIR"] = str(_cd)
                pipe_cfg["LOG_LEVEL"] = "LOG_NONE"
                pipe_cfg["ENABLE_MMAP"] = "YES"
            except Exception:
                pass
            
            # Merge inference properties
            pipe_cfg.update(inference_props)

            try:
                if dev_str.startswith("HETERO:") or dev_str.startswith("MULTI:") or dev_str.startswith("AUTO:"):
                    devs = dev_str.split(":",1)[1] if (":" in dev_str) else ""
                    pipe_cfg["MODEL_DISTRIBUTION_POLICY"] = "PIPELINE_PARALLEL"
                    if devs:
                        pipe_cfg["MULTI_DEVICE_PRIORITIES"] = devs
                        pipe_cfg["DEVICE_PRIORITIES"] = devs
            except Exception:
                pass
            try:
                if config:
                    mpl = config.get("max_prompt_len")
                    mrl = config.get("min_response_len")
                    if mpl:
                        pipe_cfg["MAX_PROMPT_LEN"] = int(mpl)
                    if mrl:
                        pipe_cfg["MIN_RESPONSE_LEN"] = int(mrl)
            except Exception:
                pass
            obj = ov_genai.LLMPipeline(str(target_dir), dev_str, pipe_cfg)
            try:
                setattr(obj, "_af_device_real", dev_str)
            except Exception:
                pass
            return obj
        try:
            if device.startswith("HETERO:"):
                devs = device.split(":",1)[1]
                # map generic GPU to Intel iGPU first when available
                try:
                    from openvino import Core as _Core
                    _hc = _Core()
                    ordered_gpus = _ordered_gpu_list(_hc)
                    igpu = ordered_gpus[0] if ordered_gpus else None
                    parts = [d.strip() for d in devs.split(",") if d.strip()]
                    mapped = []
                    for d in parts:
                        if d == "GPU" and igpu:
                            mapped.append(igpu)
                        else:
                            mapped.append(d)
                    try:
                        has_gpu = any(x.startswith("GPU") for x in mapped)
                        has_npu = any(x.startswith("NPU") for x in mapped)
                        has_cpu = any(x == "CPU" for x in mapped)
                        if has_gpu and has_npu:
                            pref_gpu = [x for x in mapped if x.startswith("GPU")]
                            pref_npu = [x for x in mapped if x.startswith("NPU")]
                            rest_cpu = ["CPU"] if has_cpu else []
                            mapped = pref_gpu + pref_npu + rest_cpu
                    except Exception:
                        pass
                    devs = ",".join(mapped)
                    try:
                        _hc.set_property("HETERO", {"MODEL_DISTRIBUTION_POLICY": "PIPELINE_PARALLEL"})
                    except Exception:
                        pass
                except Exception:
                    pass
                try:
                    p = _try(f"HETERO:{devs}")
                except Exception as e:
                    try:
                        p = _try(f"MULTI:{devs}")
                    except Exception:
                        p = None
                    if p is None:
                        try:
                            from openvino import Core as _Core
                            _c = _Core()
                            _c.set_property("AUTO", {"PERFORMANCE_HINT": perf_mode, "MODEL_DISTRIBUTION_POLICY": "PIPELINE_PARALLEL"})
                        except Exception:
                            pass
                        try:
                            p = _try(f"AUTO:{devs}")
                        except Exception:
                            p = None
                            raise e
            elif device.startswith("MULTI:"):
                devs = device.split(":",1)[1]
                # map to HETERO with pipeline parallelism, GPU prioritized to Intel iGPU
                try:
                    from openvino import Core as _Core
                    _hc = _Core()
                    ordered_gpus = _ordered_gpu_list(_hc)
                    igpu = ordered_gpus[0] if ordered_gpus else None
                    parts = [d.strip() for d in devs.split(",") if d.strip()]
                    mapped = []
                    for d in parts:
                        if d == "GPU" and igpu:
                            mapped.append(igpu)
                        else:
                            mapped.append(d)
                    try:
                        has_gpu = any(x.startswith("GPU") for x in mapped)
                        has_npu = any(x.startswith("NPU") for x in mapped)
                        has_cpu = any(x == "CPU" for x in mapped)
                        if has_gpu and has_npu:
                            pref_gpu = [x for x in mapped if x.startswith("GPU")]
                            pref_npu = [x for x in mapped if x.startswith("NPU")]
                            rest_cpu = ["CPU"] if has_cpu else []
                            mapped = pref_gpu + pref_npu + rest_cpu
                    except Exception:
                        pass
                    devs = ",".join(mapped)
                    try:
                        _hc.set_property("HETERO", {"MODEL_DISTRIBUTION_POLICY": "PIPELINE_PARALLEL"})
                    except Exception:
                        pass
                except Exception:
                    pass
                try:
                    p = _try(f"HETERO:{devs}")
                except Exception as e:
                    # fallback to individual devices in order
                    order = [d.strip() for d in devs.split(",") if d.strip()]
                    for d in order:
                        try:
                            p = _try(d)
                            break
                        except Exception:
                            p = None
                    if p is None:
                        raise e
            elif device.startswith("AUTO"):
                devs = None
                if ":" in device:
                    devs = device.split(":",1)[1]
                else:
                    try:
                        from openvino import Core
                        c = Core()
                        avail = c.available_devices
                        prio = []
                        ordered_gpus = _ordered_gpu_list(c)
                        if any(d.startswith("NPU") for d in avail):
                            prio.append("NPU")
                        if ordered_gpus:
                            prio.extend(ordered_gpus)
                        if "CPU" in avail:
                            prio.append("CPU")
                        devs = ",".join(prio) if prio else None
                    except Exception:
                        devs = None
                if devs:
                    # os.environ.setdefault("AUTO_DEVICE_PRIORITY", devs) - REMOVED
                    if perf_mode in ("THROUGHPUT", "CUMULATIVE_THROUGHPUT"):
                        # os.environ.setdefault("OV_HINT_NUM_REQUESTS", "4") - REMOVED
                        if "NUM_REQUESTS" not in inference_props:
                             inference_props["NUM_REQUESTS"] = "4"
                    try:
                        from openvino import Core
                        c = Core()
                        c.set_property("AUTO", {"PERFORMANCE_HINT": perf_mode, "MODEL_DISTRIBUTION_POLICY": "PIPELINE_PARALLEL"})
                    except Exception:
                        pass
                    try:
                        p = _try(f"AUTO:{devs}")
                    except Exception as e:
                        order = [d.strip() for d in devs.split(",") if d.strip()]
                        for d in order:
                            try:
                                p = _try(d)
                                break
                            except Exception:
                                p = None
                        if p is None:
                            raise e
                else:
                    try:
                        p = _try("AUTO")
                    except Exception:
                        p = _try("CPU")
            else:
                p = _try(device)
            if p:
                try:
                    setattr(p, "_af_device", device)
                except Exception:
                    pass
        except Exception as e:
            msg = str(e)
            order = []
            try:
                from openvino import Core
                core = Core()
                order = core.available_devices
            except Exception:
                order = ["GPU","CPU"]
            for d in order:
                try:
                    p = _try(d)
                    break
                except Exception:
                    p = None
            if p is None:
                raise
        _pipe_cache[key] = p
    return p

def load_t2i_pipeline(model_dir: Path, devices: dict | str, props: dict | None = None):
    import openvino_genai as ov_genai
    import os
    from pathlib import Path as _P
    os.environ.setdefault("OPENVINO_LOG_LEVEL", "0")
    cfg = {}
    try:
        _base = os.environ.get("AIFUNLAND_CACHE_DIR") or str(_P.cwd() / "tmp")
        _cd = _P(_base) / "ov_cache"
        _cd.mkdir(parents=True, exist_ok=True)
        cfg["CACHE_DIR"] = str(_cd)
        cfg["PERFORMANCE_HINT"] = "LATENCY"
        cfg["NUM_STREAMS"] = 1
        cfg["LOG_LEVEL"] = "LOG_NONE"
    except Exception:
        pass
    if props:
        cfg.update(props)
    if isinstance(devices, dict):
        te = devices.get("text_encoder") or devices.get("te") or devices.get("txt") or devices.get("text") or devices.get("TEXT_ENCODER") or devices.get("TEXT")
        un = devices.get("unet") or devices.get("UNET")
        vd = devices.get("vae_decoder") or devices.get("vae") or devices.get("VAE_DECODER") or devices.get("VAE")
        chosen = (str(te or "CPU"), str(un or te or "CPU"), str(vd or un or te or "CPU"))
        key = (str(model_dir),) + chosen
        p = _t2i_cache.get(key)
        if p is None:
            p = ov_genai.Text2ImagePipeline(str(model_dir))
            tried = []
            def attempt(dev_triplet):
                nonlocal p
                tried.append(dev_triplet)
                try:
                    p.compile(dev_triplet[0], dev_triplet[1], dev_triplet[2], config=cfg)
                    return True
                except Exception:
                    return False
            combos = [
                chosen,
                (chosen[0], "GPU", chosen[2] if chosen[2] != "GPU" else "CPU"),
                (chosen[0], "CPU", chosen[2]),
                ("CPU", "GPU", "GPU"),
                ("CPU", "CPU", "GPU"),
                ("CPU", "CPU", "CPU"),
            ]
            ok = False
            for c in combos:
                if attempt(c):
                    ok = True
                    key = (str(model_dir),) + c
                    break
            if not ok:
                p = ov_genai.Text2ImagePipeline(str(model_dir), str(un or te or vd or "CPU"))
            _t2i_cache[key] = p
        return p
    else:
        dev = str(devices or "CPU")
        key = (str(model_dir), dev)
        p = _t2i_cache.get(key)
        if p is None:
            try:
                p = ov_genai.Text2ImagePipeline(str(model_dir))
                p.compile(dev, dev, dev, config=cfg)
            except Exception:
                p = ov_genai.Text2ImagePipeline(str(model_dir), dev)
            _t2i_cache[key] = p
        return p

def t2i_generate(pipe, prompt: str, width: int | None = None, height: int | None = None, steps: int | None = None, guidance_scale: float | None = None):
    kwargs = {}
    if width:
        kwargs["width"] = int(width)
    if height:
        kwargs["height"] = int(height)
    if steps:
        kwargs["num_inference_steps"] = int(steps)
    if guidance_scale is not None:
        kwargs["guidance_scale"] = float(guidance_scale)
    return pipe.generate(prompt, **kwargs)

def is_model_in_use(model_dir: Path) -> bool:
    s = str(model_dir)
    for (md, _dev), _p in list(_pipe_cache.items()):
        if md == s:
            return True
    return False

def release_model(model_dir: Path):
    s = str(model_dir)
    for k in list(_pipe_cache.keys()):
        if k[0] == s:
            try:
                _pipe_cache[k] = None
            except Exception:
                pass
            try:
                del _pipe_cache[k]
            except Exception:
                pass

def is_model_loaded(model_dir: Path, device: str) -> bool:
    return _pipe_cache.get((str(model_dir), device)) is not None

def generate(pipe, prompt: str, config: dict):
    if config:
        gen = pipe.get_generation_config()
        if "max_new_tokens" in config:
            gen.max_new_tokens = int(config["max_new_tokens"])
        if "temperature" in config:
            gen.temperature = float(config["temperature"])
        if "top_k" in config:
            gen.top_k = int(config["top_k"])
        if "top_p" in config:
            gen.top_p = float(config["top_p"])
        if "repetition_penalty" in config:
            gen.repetition_penalty = float(config["repetition_penalty"])
        try:
            try:
                res = pipe.generate(prompt, gen)
            except Exception:
                res = pipe.generate([prompt], gen)
        except RuntimeError as e:
            if "bad allocation" in str(e):
                raise RuntimeError("Memory allocation failed (bad allocation). The model is likely too large for your device's memory. Please try a smaller model (e.g. INT4 quantized), reduce max_new_tokens, or switch to a device with more memory.") from e
            raise e
    else:
        try:
            try:
                res = pipe.generate(prompt)
            except Exception:
                res = pipe.generate([prompt])
        except RuntimeError as e:
            if "bad allocation" in str(e):
                raise RuntimeError("Memory allocation failed (bad allocation). The model is likely too large for your device's memory. Please try a smaller model (e.g. INT4 quantized), reduce max_new_tokens, or switch to a device with more memory.") from e
            raise e
    try:
        text = res.text if hasattr(res, "text") else (res[0] if isinstance(res, (list, tuple)) and len(res) > 0 else str(res))
    except Exception:
        text = str(res)
    metrics = None
    try:
        pm = getattr(res, "perf_metrics", None)
        if pm is not None:
            metrics = {
                "generate_ms": float(getattr(pm.get_generate_duration(), "mean", None) or 0.0),
                "ttft_ms": float(getattr(pm.get_ttft(), "mean", None) or 0.0),
                "tpot_ms": float(getattr(pm.get_tpot(), "mean", None) or 0.0),
                "throughput_tps": float(getattr(pm.get_throughput(), "mean", None) or 0.0),
            }
    except Exception:
        metrics = None
    return text, metrics


 


 
 

 
 

def generate_stream(pipe, prompt: str, config: dict, streamer):
    if config:
        gen = pipe.get_generation_config()
        if "max_new_tokens" in config:
            gen.max_new_tokens = int(config["max_new_tokens"])
        if "temperature" in config:
            gen.temperature = float(config["temperature"])
        if "top_k" in config:
            gen.top_k = int(config["top_k"])
        if "top_p" in config:
            gen.top_p = float(config["top_p"])
        if "repetition_penalty" in config:
            gen.repetition_penalty = float(config["repetition_penalty"])
        try:
            res = pipe.generate(prompt, gen, streamer=streamer)
        except RuntimeError as e:
            if "bad allocation" in str(e):
                raise RuntimeError("Memory allocation failed (bad allocation).") from e
            # If it's not bad allocation, maybe it's a config issue, try fallback
            try:
                res = pipe.generate(prompt, streamer=streamer)
            except RuntimeError as e2:
                if "bad allocation" in str(e2):
                     raise RuntimeError("Memory allocation failed (bad allocation).") from e2
                raise e2
        except Exception:
             res = pipe.generate(prompt, streamer=streamer)
    else:
        try:
            res = pipe.generate(prompt, streamer=streamer)
        except RuntimeError as e:
            if "bad allocation" in str(e):
                raise RuntimeError("Memory allocation failed (bad allocation).") from e
            raise e
    try:
        text = res.text if hasattr(res, "text") else (res[0] if isinstance(res, (list, tuple)) and len(res)>0 else str(res))
    except Exception:
        text = str(res)
    metrics = None
    try:
        pm = getattr(res, "perf_metrics", None)
        if pm is not None:
            metrics = {
                "generate_ms": float(getattr(pm.get_generate_duration(), "mean", None) or 0.0),
                "ttft_ms": float(getattr(pm.get_ttft(), "mean", None) or 0.0),
                "tpot_ms": float(getattr(pm.get_tpot(), "mean", None) or 0.0),
                "throughput_tps": float(getattr(pm.get_throughput(), "mean", None) or 0.0),
            }
    except Exception:
        metrics = None
    return text, metrics

def web_search(query: str, max_results: int = 5):
    items = []
    try:
        from duckduckgo_search import DDGS
        with DDGS() as ddgs:
            for r in ddgs.text(query, max_results=max_results):
                items.append({
                    "title": r.get("title"),
                    "url": r.get("href") or r.get("url"),
                    "snippet": r.get("body")
                })
    except Exception:
        try:
            from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
            w = DuckDuckGoSearchAPIWrapper()
            for r in w.results(query, max_results=max_results):
                items.append({
                    "title": r.get("title"),
                    "url": r.get("link"),
                    "snippet": r.get("snippet") or r.get("body")
                })
        except Exception:
            items = []
    return items

def augment_with_sources(prompt: str, sources: list[dict], lang: str = "zh"):
    lines = []
    if lang == "zh":
        lines.append("请基于以下网络检索资料进行分析并回答：")
    else:
        lines.append("Please analyze and answer using the following web sources:")
    for i, s in enumerate(sources[:5], 1):
        t = str(s.get("title") or "")
        u = str(s.get("url") or "")
        sn = str(s.get("snippet") or "")
        lines.append(f"[{i}] {t} \n{u} \n{sn}")
    if lang == "zh":
        lines.append("问题：")
    else:
        lines.append("Question:")
    lines.append(prompt)
    if lang == "zh":
        lines.append("要求：先分析再给出结论，并在<final>中输出答案。")
    else:
        lines.append("Instruction: reason first, then output the final answer in <final>.")
    return "\n".join(lines)

def quantize_model(model_dir: Path, save_dir: Path, mode: str = "int8", params: dict | None = None):
    from optimum.intel.openvino import OVModelForCausalLM
    from optimum.intel.openvino import OVWeightQuantizationConfig
    mmode = str(mode).lower()
    # if mmode != "int8":
    #     raise ValueError("int4_disabled")
    src_dir = model_dir
    try:
        if not (src_dir / "openvino_model.xml").exists():
            cand = model_dir.parent / (model_dir.name + "_ov_fp32")
            if (cand / "openvino_model.xml").exists():
                src_dir = cand
            else:
                try:
                    export_model_ir(model_dir, cand)
                    src_dir = cand if (cand / "openvino_model.xml").exists() else model_dir
                except Exception:
                    src_dir = model_dir
    except Exception:
        src_dir = model_dir
    
    bits = 4 if mmode == "int4" else 8
    # Use symmetric quantization for INT4 for better NPU compatibility if needed, 
    # but standard OVWeightQuantizationConfig(bits=4) is safe.
    qc = OVWeightQuantizationConfig(bits=bits)
    
    m = OVModelForCausalLM.from_pretrained(str(src_dir), quantization_config=qc, trust_remote_code=True)
    save_dir.mkdir(parents=True, exist_ok=True)
    m.save_pretrained(str(save_dir))
    
    # Cleanup to ensure file handles are released for deletion
    del m
    import gc
    gc.collect()

    try:
        xml = save_dir / "openvino_model.xml"
        binf = save_dir / "openvino_model.bin"
        need_cli = (not xml.exists()) or (not binf.exists())
        if not need_cli:
            try:
                need_cli = (binf.stat().st_size <= 0)
            except Exception:
                need_cli = True
        if need_cli:
            import sys, os, subprocess
            exe = sys.executable
            env = {**os.environ}
            cmd = [
                exe, "-m", "optimum.exporters.openvino.convert",
                "--model", str(src_dir),
                "--output", str(save_dir),
                "--task", "text-generation-with-past",
                "--library", "transformers",
                "--trust-remote-code",
                "--weight-format", mmode,
            ]
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)
    except Exception:
        pass
    try:
        from shutil import copyfile
        for n in ("openvino_tokenizer.xml", "openvino_detokenizer.xml"):
            fp = src_dir / n
            if fp.exists():
                copyfile(str(fp), str(save_dir / n))
        for f in ("tokenizer.json", "tokenizer_config.json", "vocab.json", "merges.txt", "special_tokens_map.json"):
            try:
                sp = src_dir / f
                if sp.exists():
                    copyfile(str(sp), str(save_dir / f))
            except Exception:
                pass
    except Exception:
        pass
    return str(save_dir)

def export_model_ir(model_dir: Path, save_dir: Path):
    from optimum.intel.openvino import OVModelForCausalLM
    import shutil, os, sys, subprocess
    save_dir.mkdir(parents=True, exist_ok=True)
    try:
        try:
            from transformers import AutoConfig
            cfg = AutoConfig.from_pretrained(str(model_dir), trust_remote_code=True)
            try:
                setattr(cfg, "use_cache", False)
            except Exception:
                pass
        except Exception:
            cfg = None
        m = OVModelForCausalLM.from_pretrained(
            str(model_dir), export=True, trust_remote_code=True, attn_implementation="eager", config=cfg
        )
        m.save_pretrained(str(save_dir))
    except Exception:
        exe = sys.executable
        env = {**os.environ, "HF_ATTENTION_IMPLEMENTATION": "eager"}
        cmd = [
            exe, "-m", "optimum.exporters.openvino.convert",
            "--model", str(model_dir),
            "--output", str(save_dir),
            "--task", "text-generation-with-past",
            "--library", "transformers",
            "--trust-remote-code"
        ]
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        except Exception as e2:
            raise e2
    for f in ("tokenizer.json", "tokenizer_config.json", "vocab.json", "merges.txt", "special_tokens_map.json"):
        try:
            src = model_dir / f
            if src.exists():
                shutil.copy(src, save_dir / f)
        except Exception:
            pass
    return str(save_dir)